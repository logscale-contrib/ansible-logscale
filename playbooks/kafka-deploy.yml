- name: Kafka
  hosts: kafka
  become: true
  pre_tasks:
    - name: Import azul key from a url
      ansible.builtin.rpm_key:
        state: present
        key: https://www.azul.com/wp-content/uploads/2021/05/0xB1998361219BD9C9.txt
      tags:
        - openjdk
    - name: Install the azul rpm from a remote repo
      ansible.builtin.dnf:
        name: "https://cdn.azul.com/zulu/bin/zulu-repo-1.0.0-1.noarch.rpm"
        state: present
      tags:
        - openjdk
    - name: Install the zulu-17 rpm from a remote repo
      notify: 
        - ansible-role-kafka:Restart Kafka
      ansible.builtin.dnf:
        name: zulu17-jdk-headless-17.0.12-1
        state: present
      tags:
        - openjdk
    - name: Add another bin dir to system-wide $PATH.
      ansible.builtin.copy:
        owner: root
        group: root
        dest: /etc/profile.d/azul.sh
        content: "PATH=$PATH:/usr/lib/jvm/zulu17/bin"
  roles:
    - role: ansible-role-kafka
      # setting kafka_node_id in play is only valid if you have 1 kafka node
      # if you have multiple kafka nodes you need to set unique kafka_node_id for each node
      kafka_topics:
        - name: "{{ logscale_kafka_topic_prefix }}-ingest"
          replication_factor: 2
          partitions: 480
          config:
            max_message_bytes: 10485760
            retention_ms: 172800000
            retention_bytes: -1
            cleanup_policy: "delete"
            compression_type: producer
        - name: "{{ logscale_kafka_topic_prefix }}-global"
          replication_factor: 2
          partitions: 1
          config:
            max_message_bytes: 2097152
            retention_bytes: 2500000000
            retention_ms: -1
            cleanup_policy: "delete"
            compression_type: producer
        - name: "{{ logscale_kafka_topic_prefix }}-transientChatter"
          replication_factor: 2
          partitions: 10
          config:
            retention_ms: 3600000
            retention_bytes: -1
            cleanup_policy: "delete"
            compression_type: producer
      # adding config that is not defined by role
      kafka_additional_config:
        min.insync.replicas: 2
        default.replication.factor: 3
        offsets.topic.replication.factor: 3
        transaction.state.log.min.isr: 2
        transaction.state.log.replication.factor: 3

        # num.recovery.threads.per.data.dir: 2  # default is 1 set to number of cores in brokers
        # num.recovery.threads.per.data.dir: 1
        # For storage where storage is thin provisioned enable
        # log.preallocate: true
        # background threads include replication for very high throughput increase based on available CPU cores and usage default is 10
        # background.threads: 20
        # num.io.threads increase based on available CPU cores and usage default is 8
        # num.io.threads: 20
        # num.network.threads increase based on available CPU cores and usage default is 3
        # num.network.threads: 6
        # num.replica.fetchers increase based on available CPU cores and usage default is 1
        # num.replica.fetchers: 10

        # replica.socket.receive.buffer.bytes: 65536 increased for higher replication throughput based on memory
        # replica.socket.receive.buffer.bytes: 1000000

        # socket.receive.buffer.bytes: 102400 default setting to -1 to use OS default
        socket.receive.buffer.bytes: -1
        # socket.request.max.bytes: 104857600 default setting to 100MB probably should not change
        # socket.send.buffer.bytes: 102400 default setting to -1 to use OS default
        socket.send.buffer.bytes: -1
        # adding kafka startup options
        # max message size for all topics by default:
        message.max.bytes: 104857600
        # compression.type: producer      
      
